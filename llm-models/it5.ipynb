{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 9.37MB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 936kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 1.44MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 10.1MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 242M/242M [00:16<00:00, 14.7MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 659kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what is a cat ?\"\n",
    "input =  tokenizer(query , return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 125,   19,    3,    9, 1712,    3,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = model.generate(**input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was ist ein Katz?\n"
     ]
    }
   ],
   "source": [
    "answer = tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who is the main character of the story ?\n",
      "Answer: Lily\n"
     ]
    }
   ],
   "source": [
    "# Load model directly from Hugging Face repository\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: summarise the story \n",
      "Answer: Lily decided to uncover the secrets of the book, setting off on an adventure that would change her life forever\n"
     ]
    }
   ],
   "source": [
    "# List of question-context pairs\n",
    "question_context_pairs = [\n",
    "    (\"summarise the story \", \"Once upon a time, in a quaint village nestled between rolling hills and lush forests, there lived a young girl named Lily. Lily was known throughout the village for her insatiable curiosity and her love for exploring the world around her.One sunny morning, as she was wandering near the edge of the forest, Lily stumbled upon a mysterious old book. Its pages were filled with strange symbols and diagrams.Intrigued, Lily decided to uncover the secrets of the book, setting off on an adventure that would change her life forever.\")\n",
    "    # Add more question-context pairs as needed\n",
    "]\n",
    "\n",
    "# Generate answers for each pair\n",
    "for question, context in question_context_pairs:\n",
    "    input_text = f\"question: {question} context: {context} </s>\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=300,  truncation=True)\n",
    "    output = model.generate(input_ids,  max_length=150, min_length=5)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: translated English to German : I am good how are you ?\n",
      "Answer: : Ich bin gut wie sind Sie?\n"
     ]
    }
   ],
   "source": [
    "question = \"translated English to German : I am good how are you ?\"\n",
    "input_ids = tokenizer.encode(question, return_tensors='pt', max_length=300,  truncation=True)\n",
    "output = model.generate(input_ids,  max_length=150, min_length=5)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a long answer that\n",
      "needs to be split into\n",
      "multiple lines so that each\n",
      "line contains at most 30\n",
      "characters.\n"
     ]
    }
   ],
   "source": [
    "def print_formatted_text(text, max_line_length):\n",
    "    words = text.split()  # Split the text into words\n",
    "    current_line = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) + 1 <= max_line_length:\n",
    "            current_line += \" \" + word if current_line else word\n",
    "        else:\n",
    "            print(current_line)\n",
    "            current_line = word\n",
    "\n",
    "    if current_line:\n",
    "        print(current_line)\n",
    "\n",
    "answer = \"This is a long answer that needs to be split into multiple lines so that each line contains at most 30 characters.\"\n",
    "max_line_length = 30\n",
    "print_formatted_text(answer, max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Summarize : Fine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\n",
      "Or: What does T5 know?\n",
      "The following tutorial guides you through the process of fine-tuning a pre-trained T5 model, evaluating its accuracy, and using it for prediction, all on a free Google Cloud TPU Open In Colab.\n",
      "\n",
      "Background\n",
      "T5 was introduced in the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. In that paper, we provided a comprehensive picture of how we pre-trained a standard text-to-text Transformer model on a large text corpus, achieving state-of-the-art results on many NLP tasks after fine-tuning.\n",
      "\n",
      "We pre-trained T5 on a mixture of supervised and unsupervised tasks with the majoriy of data coming from an unlabeled dataset we developed called C4. C4 is based on a massive scrape of the web produced by Common Crawl. Loosely speaking, pre-training on C4 ideally gives T5 an understanding of natural language in addition to general world knowledge.\n",
      "\n",
      "How can we assess what T5 knows?\n",
      "As the name implies, T5 is a text-to-text model, which enables us to train it on arbitrary tasks involving a textual input and output. As we showed in our paper, a huge variety of NLP tasks can be cast in this format, including translation, summarization, and even classification and regression tasks.\n",
      "\n",
      "One way to use this text-to-text framework is on reading comprehension problems, where the model is fed some context along with a question and is trained to predict the question's answer. For example, we might feed the model the text from the Wikipedia article about Hurrican Connie along with the question \"On what date did Hurricane Connie occur?\" and train the model to predict the answer \"August 3rd, 1955\". A related task is open-domain question answering (QA) where the model is not provided with this oracle context. Typically, open-domain QA systems include a mechanism to look up information in an external knowledge source. This setting is similar to an \"open-book\" exam.\n",
      "\n",
      "In this notebook, we'll be training T5 on a variant of this task which we call closed-book question answering. In closed-book QA, we feed the model a question without any context or access to external knowledge and train it to predict the answer. Since the model doesn't receive any context, the primary way it can learn to answer these questions is based on the \"knowledge\" it obtained during pre-training. We don't expect T5 to contain super specific information, so we will be focusing on two question-answering datasets which largely include trivia questions (i.e. facts about well-known subjects). Similar investigations have recently been done to test the knowledge stored by BERT and GPT-2.\n",
      "\n",
      "T5 was not pre-trained on closed-book QA, so in this notebook we'll first create two new tasks and then use the t5 library to fine-tune, evaluate, and obtain predictions from T5. In the end, T5's performance on closed-book QA can give us a sense of what kind (and how much) information T5 managed to learn during pre-training.\n",
      "\n",
      "State-of-the-art Results\n",
      "We published a more in-depth investigation of closed-book QA with T5 where we achieved SOTA on open-domain variants of WebQuestions and TriviaQA in addition to surpisingly strong results on Natural Questions. The code in this notebook is a simplified version of those experiments but still produces good results.\n",
      "\n",
      "For code to reproduce our best results, please see the t5_closed_book_qa repo.\n",
      "\n",
      "Caveats\n",
      "While we provide instructions for running on a Cloud TPU via Colab for free, a Google Cloud Storage (GCS) bucket is required for storing model parameters and data. The GCS free tier provides 5 GB of storage, which should be enough to train the large model and smaller but not the 3B or 11B parameter models. You can use part of your initial $300 credit to get more space.\n",
      "The Cloud TPU provided by Colab (a v2-8) does not have enough memory to fine-tune the 11B parameter model. For this model, you will need to fine-tune inside of a GCP instance (see README). \n",
      "************\n",
      "\n",
      "\n",
      "Answer to question is : \n",
      " \n",
      "\n",
      "T5 on a large text corpus, achieving state-of-the-art results on many NLP tasks after fine-tuning.\n",
      "T5 is a text-to-text model, which enables us to train it on arbitrary tasks. T5 is a text-to-text\n",
      "model, which enables us to train it on arbitrary tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" Summarize : Fine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\n",
    "Or: What does T5 know?\n",
    "The following tutorial guides you through the process of fine-tuning a pre-trained T5 model, evaluating its accuracy, and using it for prediction, all on a free Google Cloud TPU Open In Colab.\n",
    "\n",
    "Background\n",
    "T5 was introduced in the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. In that paper, we provided a comprehensive picture of how we pre-trained a standard text-to-text Transformer model on a large text corpus, achieving state-of-the-art results on many NLP tasks after fine-tuning.\n",
    "\n",
    "We pre-trained T5 on a mixture of supervised and unsupervised tasks with the majoriy of data coming from an unlabeled dataset we developed called C4. C4 is based on a massive scrape of the web produced by Common Crawl. Loosely speaking, pre-training on C4 ideally gives T5 an understanding of natural language in addition to general world knowledge.\n",
    "\n",
    "How can we assess what T5 knows?\n",
    "As the name implies, T5 is a text-to-text model, which enables us to train it on arbitrary tasks involving a textual input and output. As we showed in our paper, a huge variety of NLP tasks can be cast in this format, including translation, summarization, and even classification and regression tasks.\n",
    "\n",
    "One way to use this text-to-text framework is on reading comprehension problems, where the model is fed some context along with a question and is trained to predict the question's answer. For example, we might feed the model the text from the Wikipedia article about Hurrican Connie along with the question \"On what date did Hurricane Connie occur?\" and train the model to predict the answer \"August 3rd, 1955\". A related task is open-domain question answering (QA) where the model is not provided with this oracle context. Typically, open-domain QA systems include a mechanism to look up information in an external knowledge source. This setting is similar to an \"open-book\" exam.\n",
    "\n",
    "In this notebook, we'll be training T5 on a variant of this task which we call closed-book question answering. In closed-book QA, we feed the model a question without any context or access to external knowledge and train it to predict the answer. Since the model doesn't receive any context, the primary way it can learn to answer these questions is based on the \"knowledge\" it obtained during pre-training. We don't expect T5 to contain super specific information, so we will be focusing on two question-answering datasets which largely include trivia questions (i.e. facts about well-known subjects). Similar investigations have recently been done to test the knowledge stored by BERT and GPT-2.\n",
    "\n",
    "T5 was not pre-trained on closed-book QA, so in this notebook we'll first create two new tasks and then use the t5 library to fine-tune, evaluate, and obtain predictions from T5. In the end, T5's performance on closed-book QA can give us a sense of what kind (and how much) information T5 managed to learn during pre-training.\n",
    "\n",
    "State-of-the-art Results\n",
    "We published a more in-depth investigation of closed-book QA with T5 where we achieved SOTA on open-domain variants of WebQuestions and TriviaQA in addition to surpisingly strong results on Natural Questions. The code in this notebook is a simplified version of those experiments but still produces good results.\n",
    "\n",
    "For code to reproduce our best results, please see the t5_closed_book_qa repo.\n",
    "\n",
    "Caveats\n",
    "While we provide instructions for running on a Cloud TPU via Colab for free, a Google Cloud Storage (GCS) bucket is required for storing model parameters and data. The GCS free tier provides 5 GB of storage, which should be enough to train the large model and smaller but not the 3B or 11B parameter models. You can use part of your initial $300 credit to get more space.\n",
    "The Cloud TPU provided by Colab (a v2-8) does not have enough memory to fine-tune the 11B parameter model. For this model, you will need to fine-tune inside of a GCP instance (see README). \"\"\"\n",
    "input_ids = tokenizer.encode(question, return_tensors='pt', max_length=300,  truncation=True)\n",
    "output = model.generate(input_ids,  max_length=550, min_length=5)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Question:\", question)\n",
    "print(\"***\"* 4)\n",
    "print(\"\\n\\nAnswer to question is : \\n \\n\")\n",
    "print_formatted_text(answer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
